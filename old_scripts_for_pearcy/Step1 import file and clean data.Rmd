---
title: "Step1 import files and clean data"
author: "Jackie Ouzman"
date: "12 August 2019"
output: html_document
---

# SAMDBNRMB: Applying virtual fencing for NRM outcomes in SA
** Project details: **

Start Date	6/05/2019

End Date 31/08/2019

Team Member:
Rick Llewellyn (4 days)
Damian Mowat (20 days)
Willie Shoobridge (10 days)
Jackie Ouzman (10 days)


Jim Lea Research Technician wk  02 67761419 mob  0407137466

## Aim of project:
Use virtual fencing collar on cattle to protect watercourse within a paddock. Over 8 weeks observe the reduced grazing pressure in the exclusion zone.

## Background
### site: Eden Valley
### Animals:
### Collar: make and model used






```{r setup install packages, include=FALSE}
library(tidyverse)
library(dplyr)
library(lubridate)
library(readxl)

library(ggplot2)
library(readr)
library(sp)
#library(biogeo)
library(stringr)
library(rgdal)
library(sf)

#library(gganimate)
#library(png)
#library(gifski)

knitr::opts_chunk$set(include = FALSE)
```

## The first steps is to bring in the data files

After the files are downloaded from the collar they get 'converted'
Jim Lea from Armidale does this.
The files get a folder with a date and within this is a series of csv files.
One file for each collar each day.

At the monmnet the data (for the older collars are located here:

W:/VF/Eden_Valley/logged_VF_data/collar logs_download2/

# step 1 create function to import the data.
This function attaches the collar name to the file
creates a sensiable time column and writes the files out to another directory.
Note this could be changes if you are working on a vitrual machine.

I have no idea why this won't run on Pearcey in the Rmarkdown enviroment - it works as a stand alone script
I have messed around with the setting the working directory etc...

```{r setting up functions to import the data, eval=FALSE, include=FALSE}
read_csv_FUN <- function(file ){
  the_data <- read_csv(file, col_types = cols(value = col_character()))
}


import_function <- function(mydir){
  
  myfiles = list.files(path=mydir, pattern="*.csv", full.names=TRUE)
  #1a########### Get file names
  filenames <- myfiles
  collars <- str_extract(myfiles, "[a-z]+\\d{1,6}")
  
  #1b########## Get length of each csv
  file_lengths <- unlist(lapply(lapply(filenames, read_csv), nrow))
  #file_lengths
  #1c########## Repeat collars using lengths
  file_names <- rep(collars,file_lengths)
  
  #1d######### Create table
  tbl <- lapply(filenames, read_csv_FUN) %>% #this call the function read_csv_FUN I made outside this import_function
    bind_rows()
  #1e######### Combine file_names and tbl
  VF <- cbind(tbl, collar_ID = file_names)
  #glimpse(VF)
  
  ################## Step 2 extra clms for  raw logged data   ##############################################
  VF <- VF %>% 
    separate(collar_ID,into =  c("collar", "date"),  sep = "_", remove = FALSE ) %>% 
    mutate(hms = hms::as.hms(time, tz="GMT"),
           date= date(time),
           month = month(time),
           day = day(time))
  #write_csv(VF, path = paste0("W:/VF/Eden_Valley/logged_VF_data/download2_R_output/", "VF_", mydir, ".csv")) 
}


```

# step 2 Use the function to import the data and combine
It would be better if this was a loop.


```{r use function bring n data, eval=FALSE, include=FALSE}
##### Use the function to bring in data for one day that is specified ######
getwd()
setwd("W:/VF/Eden_Valley/logged_VF_data/collar logs_download2/")

VF_20190607 <- import_function("20190607")
VF_20190606 <- import_function("20190606")
VF_20190605 <- import_function("20190605")
VF_20190604 <- import_function("20190604")
VF_20190603 <- import_function("20190603")
VF_20190602 <- import_function("20190602") #not written r bind 
VF_20190601 <- import_function("20190601")
VF_20190531 <- import_function("20190531")
VF_20190530 <- import_function("20190530")
VF_20190529 <- import_function("20190529")
VF_20190528 <- import_function("20190528")
VF_20190527 <- import_function("20190527")
VF_20190526 <- import_function("20190526")
VF_20190525 <- import_function("20190525")
VF_20190524 <- import_function("20190524")
VF_20190523 <- import_function("20190523")
VF_20190522 <- import_function("20190522")
VF_20190521 <- import_function("20190521")
VF_20190520 <- import_function("20190520")
VF_20190519 <- import_function("20190519")
VF_20190518 <- import_function("20190518")
VF_20190517 <- import_function("20190517")

VF_week1 <- rbind(VF_20190517, VF_20190518, VF_20190519,
                  VF_20190520, VF_20190521, VF_20190522, VF_20190523)
VF_week2 <- rbind(VF_20190524, VF_20190525, VF_20190526,
                  VF_20190527, VF_20190528, VF_20190529, VF_20190530)
VF_week3 <- rbind(VF_20190531, VF_20190601, 
                  VF_20190602, #not written ? not sure why
                  VF_20190603, VF_20190604, VF_20190605, VF_20190606)

write_csv(VF_week1, path = paste0("VF_week1.csv"))
write_csv(VF_week2, path = paste0("VF_week2.csv"))
write_csv(VF_week3, path = paste0("VF_week3.csv"))
glimpse(VF_20190607)

```

# These steps are quite long
A short cut and a way to improve running R would be just to start here with the merged files.
Note that there is an important step that might need to be revisted.

I am just using InclusionBorder_m data and setting values to number.
If I need to look at the other data types I might need to revist this step.

```{r import merged week1 - 3 files}
#file://Fssa2-adl/clw-share1/Microlab/VF/Eden_Valley/logged_VF_data/download2_R_output/VF_week1.csv
VF_week1 <- read_csv("W:/VF/Eden_Valley/logged_VF_data/download2_R_output/VF_week1.csv")
VF_week2 <- read_csv("W:/VF/Eden_Valley/logged_VF_data/download2_R_output/VF_week2.csv")
VF_week3 <- read_csv("W:/VF/Eden_Valley/logged_VF_data/download2_R_output/VF_week3.csv")

##########       Merge this all togther   ##########       
VF_week1_2_3 <- rbind(VF_week1, VF_week2, VF_week3)

#saveRDS(VF_week1_2_3,  paste0("download2_R_output/","VF_week1_2_3.rds"))

VF_week1_2_3 <- read_rds("download2_R_output/VF_week1_2_3.rds")

 #########    Remove the NA   ##########
VF_week1_2_3 <- VF_week1_2_3 %>% filter(!is.na(lat) | !is.na(lon))

summary(VF_week1_2_3$lat)
summary(VF_week1_2_3$lon)



##########       ensure the column value is a number - double    ##########
VF_week1_2_3_InclusionBord <- filter(VF_week1_2_3, event == "InclusionBorder_m") %>%   
  mutate( value = as.double(value))

saveRDS(VF_week1_2_3_InclusionBord,  "VF_week1_2_3_InclusionBord.rds")




```

#3.Do projections
 

#https://spatialreference.org/ref/epsg/gda94-mga-zone-56/
#epsg projection 28356


```{r projetion for inclusion week 1 -3 using sp}
mapCRS <- CRS("+init=epsg:28354")     # 28354 = GDA_1994_MGA_Zone_54
wgs84CRS <- CRS("+init=epsg:4326")   # 4326 WGS 84 - assumed for input lats and longs

####################  convert lat and longs to x and Y     ##########################################
coordinates(VF_week1_2_3_InclusionBord) <- ~ lon + lat
proj4string(VF_week1_2_3_InclusionBord) <- wgs84CRS   # assume input lat and longs are WGS84
#make new object_1
VF_week1_2_3_InclusionBord_1 <- spTransform(VF_week1_2_3_InclusionBord, mapCRS)
#make new df_1
head(VF_week1_2_3_InclusionBord_1,3)
VF_week1_2_3_InclusionBord = as.data.frame(VF_week1_2_3_InclusionBord_1) #this has the new coordinates projected !YES!!
#make new df with point x and point y
VF_week1_2_3_InclusionBord <- mutate(VF_week1_2_3_InclusionBord,POINT_X = lon,  POINT_Y = lat )
head(VF_week1_2_3_InclusionBord)
```

```{r projetion for inclusion week 1 -3 using sf}
head(VF_week1_2_3_InclusionBord)
eden_valley <- st_read("EdenValley_site1GDA_a.shp")
head(eden_valley)
eden_valley <- st_transform(eden_valley, crs = 28354 )
library(sf)
st_crs(eden_valley)
the_crs <- crs(eden_valley, asText = TRUE)

VF_week1_2_3_InclusionBord_1 <- st_as_sf(VF_week1_2_3_InclusionBord, coords = c("lat", "lon"), crs = 4326)
head(VF_week1_2_3_InclusionBord_1)
st_transform(VF_week1_2_3_InclusionBord_1, 28355 )

#saveRDS(VF_week1_2_3_InclusionBord_1,  "VF_week1_2_3_InclusionBord_1.rds")
```
# Time to fix up the data 
The first task with this is to remove the data points that are not in the paddock.



I can use the clip(from sf) function but I need to convert it into an sf object 
1) turn df data into sf object to do spatial analysis on.




3) use the st intersection function to clip the data to the paddock boundaries.


```{r clip_a }
eden_valley <- st_transform(eden_valley, 28355 )
head(eden_valley)
head(VF_week1_2_3_InclusionBord_1)
geo <- st_geometry(VF_week1_2_3_InclusionBord_1)
head(geo)
plot(geo)
ggplot() +
  geom_sf(data = eden_valley, color = "black", fill = NA) +
   geom_sf(data = VF_week1_2_3_InclusionBord_1_geo)
  
VF_week1_2_3_InclusionBord_clip <- st_intersection(VF_week1_2_3_InclusionBord_1, eden_valley)
```
Dont think I need this step

```{r back to df, eval=FALSE, include=FALSE}



head(VF_week1_2_3_InclusionBord_clip ,3)
df_VF_week1_2_3_InclusionBord_clip <- data.frame(VF_week1_2_3_InclusionBord_clip)
df_VF_week1_2_3_InclusionBord_clip <- mutate(df_VF_week1_2_3_InclusionBord_clip,
                                             POINT_X = lon,  POINT_Y = lat )
head(df_VF_week1_2_3_InclusionBord_clip ,3)

df_VF_week1_2_3_InclusionBord_c <- select(df_VF_week1_2_3_InclusionBord_clip,
                                             -geometry,
                                             -OID_,
                                             -Name,
                                             -FolderPath, 
                                             -SymbolID, 
                                             -AltMode, 
                                             -Base, 
                                             -Clamped, 
                                             -Extruded, 
                                             -Snippet, 
                                             -PopupInfo, 
                                             -Shape_Leng,
                                             -Shape_Area,
                                             -lon,
                                             -lat)

head(df_VF_week1_2_3_InclusionBord_c ,3)
```

# Time to fix up the start date of trial 
The first task with this is to remove the data points that are pre trial.

```{r remove pre trial values}
#############  Remove Pre trial readings ##############

df_VF_week1_2_3_InclusionBord_c <- filter(VF_week1_2_3_InclusionBord_clip, time > as_datetime('2019-05-20 12:30:00', tz="GMT"))

```

# Create new data column for animal number and iceqube (havent done this iceqube data yet)
The same animals were used for the lenghth of the trial, but at times the collars were changed.
I need to assign animal number to correct collars.- done
This log was made by Jim and Damian.

"\\FSSA2-ADL\clw-share1\Microlab\VF\Eden_Valley\logged_VF_data\collar log record complete.xlsx"
The actual times for collar strating and stopping are not included in all of the log entries.
Now to find them!

```{r animal_ID}

#test <- filter(df_VF_week1_2_3_InclusionBord_c,collar_ID == "ac220" ) 
#min(as_datetime(test$time, tz="GMT")) 
#max(as_datetime(test$time, tz="GMT"))

df_VF_week1_2_3_InclusionBord_c_animalID <- mutate(df_VF_week1_2_3_InclusionBord_c,
                                             animal_ID = case_when(
                                               collar_ID == "ac138" ~ "Q46",
                                               collar_ID == "ac187" ~ "Q36",
                                               collar_ID == "ac204" ~ "Q108",
                                               collar_ID == "ac207" ~ "Q42",
                                               collar_ID == "ac212" ~ "Q29",
                                               collar_ID == "ac213" &
                                                 between(time, as_datetime('2019-05-20 12:30:00', tz="GMT"),
                                                         as_datetime('2019-05-28 06:44:00', tz="GMT")) ~ "Q47",
                                               collar_ID == "ac320" &
                                                 between(time, as_datetime('2019-05-28 11:01:00', tz="GMT"),
                                                         as_datetime('2019-06-06 17:27:00', tz="GMT")) ~ "Q47" ,
                                               collar_ID == "ac217" ~ "Q27",
                                               collar_ID == "ac218" ~ "Q2",
                                               collar_ID == "ac219" &
                                                 between(time, as_datetime('2019-05-20 12:32:03', tz="GMT"),
                                                         as_datetime('2019-05-25 11:10:00', tz="GMT"))~ "Q10",
                                               collar_ID == "ac220" &
                                                 between(time, as_datetime('2019-05-25 11:01:00', tz="GMT"),
                                                         as_datetime('2019-06-06 17:27:18', tz="GMT"))~ "Q10",
                                               collar_ID == "ac325" ~ "Q9",
                                               collar_ID == "ac328" ~ "Q109",
                                               collar_ID == "ac331" ~ "Q51",
                                               collar_ID == "ad1945" ~ "Q28",
                                               collar_ID == "ad2042" ~ "Q26",
                                               collar_ID == "ad2043" ~ "Q75",
                                               collar_ID == "ad3374" ~ "Q11",
                                               collar_ID == "ad3396"  &
                                                 between(time, as_datetime('2019-05-20 12:30:00', tz="GMT"),
                                                         as_datetime('2019-05-27 16:19:00', tz="GMT"))~ "Q45",
                                               collar_ID == "ac209"  &
                                                 between(time, as_datetime('2019-05-28 11:11:00', tz="GMT"),
                                                         as_datetime('2019-06-06 17:00:00', tz="GMT"))~ "Q45",
                                               collar_ID == "ad3471" ~ "Q15",
                                               collar_ID == "ad3502" ~ "Q8",
                                               collar_ID == "ad3925" ~ "Q110",
                                               TRUE ~ "NA"))


```



```{r saving week1 to 3 df}
saveRDS(df_VF_week1_2_3_InclusionBord_c_animalID,  "download2_R_output/df_VF_week1_2_3_InclusionBord_c_animalID.rds")
```




This indicates that some distance measure are wrong and should be removed.
I think anything between 0 to 500 should be retained and -500 to 0

```{r remove invalid distance readings}


Check_df_VF_week1_2_3_InclusionBord_c_animalID <- filter(df_VF_week1_2_3_InclusionBord_c_animalID, 
                                                  between(value,-500, 0)|
                                                  between(value,0, 500))


#summary(Check_df_VF_week1_2_3_InclusionBord_c_animalID)

 saveRDS(Check_df_VF_week1_2_3_InclusionBord_c_animalID,  "download2_R_output/df_VF_week1_2_3_InclusionBord_c_animalID_clean.rds")                                             

```



This is the final product.
It is the inclusion data that has X and Y coords,clipped to paddock bounadries, has animal id an distance values greater/ less than -500 and 500 removed
df_VF_week1_2_3_InclusionBord_c_animalID_clean.rds


This indicated that we are missing some animal ID values...
```{r check of final}
head(Check_df_VF_week1_2_3_InclusionBord_c_animalID)
with(Check_df_VF_week1_2_3_InclusionBord_c_animalID, table(date, animal_ID))
```


